{"cells":[{"cell_type":"markdown","source":["<a href=\"http://www.calstatela.edu/centers/hipic\"><img align=\"left\" src=\"https://avatars2.githubusercontent.com/u/4156894?v=3&s=100\"><image/>\n</a>\n<img align=\"right\" alt=\"California State University, Los Angeles\" src=\"http://www.calstatela.edu/sites/default/files/groups/California%20State%20University%2C%20Los%20Angeles/master_logo_full_color_horizontal_centered.svg\" style=\"width: 360px;\"/>\n\n# CIS5560 Term Project Tutorial"],"metadata":{}},{"cell_type":"markdown","source":["------\n#### Authors: [Anupam Sahay](https://www.linkedin.com/in/anupam-sahay-9514a349/); [Krithy Nanaiah Atrangada];[Neha Shashidhara Guli]\n\n#### Instructor: [Jongwook Woo](https://www.linkedin.com/in/jongwook-woo-7081a85)\n\n#### Date: 05/16/2018"],"metadata":{}},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('problem').getOrCreate()"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["from pyspark.sql import SQLContext\nfrom pyspark.sql.types import *\nsqlContext = SQLContext(sc)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["%fs ls /FileStore/tables/weather.csv"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["%fs ls /FileStore/tables/status.csv"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["%fs ls /FileStore/tables/trip.csv"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["%fs ls /FileStore/tables/station.csv"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["status_data = spark.read.csv(\"dbfs:/FileStore/tables/status.csv\", inferSchema=True, header=True)\ndisplay(status_data)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["weather_data = spark.read.csv(\"dbfs:/FileStore/tables/weather.csv\", inferSchema=True, header=True)\ndisplay(weather_data)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["trip_data = spark.read.csv(\"dbfs:/FileStore/tables/trip.csv\", inferSchema=True, header=True)\ndisplay(trip_data)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["station_data = spark.read.csv(\"dbfs:/FileStore/tables/station.csv\", inferSchema=True, header=True)\ndisplay(station_data)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["trip_data.show()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["from pyspark.sql.functions import isnan, count, when\nstatus_data.select([count(when(isnan(c), c)).alias(c) for c in status_data.columns]).show()"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["from pyspark.sql.functions import isnan, count, when\ntrip_data.select([count(when(isnan(c), c)).alias(c) for c in trip_data.columns]).show()"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["from pyspark.sql.functions import mean, min, max, stddev\ntrip_data.select([mean('duration'), min('duration'), max('duration'), stddev('duration')]).show()"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["#creating a new column \ntrip_data = trip_data.withColumn('duration_new', trip_data.duration / 60)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["#dropping old column..\ntrip_data = trip_data.drop(trip_data.duration)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["trip_data.show()"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["trip_data.select([mean('duration_new'), min('duration_new'), max('duration_new'), stddev('duration_new')]).show()"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["trip_data.count()"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["trip_data = trip_data.filter(trip_data['duration_new'] <= 360)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["trip_data.count()"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["trip_data.show()"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["trip_data = trip_data.toPandas()"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["trip_data.shape"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["#Convert to datetime so that it can be manipulated more easily\nimport pandas as pd\ntrip_data.start_date = pd.to_datetime(trip_data.start_date, format='%m/%d/%Y %H:%M')\ntrip_data['date'] = trip_data.start_date.dt.date\ndates = {}\nfor d in trip_data.date:\n    if d not in dates:\n        dates[d] = 1\n    else:\n        dates[d] += 1\ntrain = pd.DataFrame(dates.items(), columns=['date', 'trips'])"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["train = sqlContext.createDataFrame(train)"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["train.show()"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["train = train.sort('date',ascending = True)"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["train.show()"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["weather_data.show()"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["weather_data.select([count(when(isnan(c), c)).alias(c) for c in weather_data.columns]).show()"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["weather_data = weather_data.toPandas()\nweather_data.date = pd.to_datetime(weather_data.date, format='%m/%d/%Y')\nweather_data = sqlContext.createDataFrame(weather_data)"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["print(train.count())\nprint(weather_data.count())"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["weather_data.select('zip_code').distinct().show()"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["weather_data = weather_data.filter(weather_data['zip_code'] == 94107)"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["weather_data.select('events').distinct().show()"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["weather_data = weather_data.na.fill('Normal')\nweather_data = weather_data.na.replace('rain','Rain')"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["weather_data.select('events').distinct().show()"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["weather_data = weather_data.toPandas()\nevents = pd.get_dummies(weather_data.events)\nweather_data = weather_data.merge(events, left_index = True, right_index = True)\n#Remove features we don't need\nweather_data = weather_data.drop(['events','zip_code'],1)"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["weather_data = sqlContext.createDataFrame(weather_data)"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["weather_data = weather_data.na.drop()"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["weather_data.select('precipitation_inches').show(700)"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["weather_data = weather_data.toPandas()\nweather_data.precipitation_inches = pd.to_numeric(weather_data.precipitation_inches, errors = 'coerce')\nweather_data.loc[weather_data.precipitation_inches.isnull(), \n            'precipitation_inches'] = weather_data[weather_data.precipitation_inches.notnull()].precipitation_inches.median()\nweather_data.date = pd.to_datetime(weather_data.date, format='%m/%d/%Y %H:%M')\nweather_data['date'] = weather_data.date.dt.date\nweather_data = sqlContext.createDataFrame(weather_data)"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["train1 = train.join(weather_data, on=['date'], how='left_outer')"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["train1.show()"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["station_data.show()"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"code","source":["#Good, each stations is only listed once\nprint(station_data.distinct().count())\nprint(station_data.count())"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"code","source":["station_data = station_data.toPandas()\ntrain1 = train1.toPandas()\nstation_data.installation_date = pd.to_datetime(station_data.installation_date, format = \"%m/%d/%Y\").dt.date\ntotal_docks = []\nfor day in train1.date:\n    total_docks.append(sum(station_data[station_data.installation_date <= day].dock_count))\ntrain1['total_docks'] = total_docks\nstation_data = sqlContext.createDataFrame(station_data)\ntrain1 = sqlContext.createDataFrame(train1)"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"code","source":["train1.show()"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"code","source":["#Find all of the holidays during our time span\nfrom pandas.tseries.holiday import USFederalHolidayCalendar\nfrom pandas.tseries.offsets import CustomBusinessDay\ntrain = train1.toPandas()\ncalendar = USFederalHolidayCalendar()\nholidays = calendar.holidays(start=train.date.min(), end=train.date.max())\n\nus_bd = CustomBusinessDay(calendar=USFederalHolidayCalendar())\nbusiness_days = pd.DatetimeIndex(start=train.date.min(), end=train.date.max(), freq=us_bd)\n\nbusiness_days = pd.to_datetime(business_days, format='%Y/%m/%d').date\nholidays = pd.to_datetime(holidays, format='%Y/%m/%d').date\n\n#A 'business_day' or 'holiday' is a date within either of the respected lists.\ntrain['business_day'] = train.date.isin(business_days)\ntrain['holiday'] = train.date.isin(holidays)\n\n#Convert True to 1 and False to 0\ntrain.business_day = train.business_day.map(lambda x: 1 if x == True else 0)\ntrain.holiday = train.holiday.map(lambda x: 1 if x == True else 0)\ntrain['year'] = pd.to_datetime(train['date']).dt.year\ntrain['month'] = pd.to_datetime(train['date']).dt.month\ntrain['weekday'] = pd.to_datetime(train['date']).dt.weekday\nlabels = train.trips\ntrain = train.drop(['date'], 1)\ntrain = sqlContext.createDataFrame(train)"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"code","source":["train.show()"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"code","source":["train = train.withColumnRenamed('trips','target')\ntrain = train.na.drop()"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\nassembler = VectorAssembler(inputCols=[\n 'max_temperature_f',\n 'mean_temperature_f',\n 'min_temperature_f',\n 'max_dew_point_f',\n 'mean_dew_point_f',\n 'min_dew_point_f',\n 'max_humidity',\n 'mean_humidity',\n 'min_humidity',\n 'max_sea_level_pressure_inches',\n 'mean_sea_level_pressure_inches',\n 'min_sea_level_pressure_inches',\n 'max_visibility_miles',\n 'mean_visibility_miles',\n 'min_visibility_miles',\n 'max_wind_Speed_mph',\n 'mean_wind_speed_mph',\n 'max_gust_speed_mph',\n 'precipitation_inches',\n 'cloud_cover',\n 'wind_dir_degrees',\n 'Fog',\n 'Fog-Rain',\n 'Normal',\n 'Rain',\n 'Rain-Thunderstorm',\n 'total_docks',\n 'business_day',\n 'holiday',\n 'year',\n 'month',\n 'weekday'],\n outputCol='features')"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"code","source":["output = assembler.transform(train)"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"code","source":["final_data = output.select(['features','target'])\nfinal_data.show()"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"code","source":["#split data\ntrain,test = final_data.randomSplit([0.8,0.2])"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"code","source":["from pyspark.ml.regression import (DecisionTreeRegressor)\ndTree = DecisionTreeRegressor(labelCol='target', featuresCol='features')"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"code","source":["dtc_model = dTree.fit(train)\ndtc_pred = dtc_model.transform(test)"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"code","source":["from pyspark.ml.evaluation import RegressionEvaluator\n# Select (prediction, true label) and compute test error\nevaluator = RegressionEvaluator(\n    labelCol=\"target\", predictionCol=\"prediction\", metricName=\"r2\")\naccuracy = evaluator.evaluate(dtc_pred)\nprint(\"Accuracy on test data = %g\" % (accuracy * 100))\n\nevaluator1 = RegressionEvaluator(\n    labelCol=\"target\", predictionCol=\"prediction\", metricName=\"rmse\")\nrmse = evaluator1.evaluate(dtc_pred)\nprint(\"RMSE = %g\" % (rmse))"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"code","source":["from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nparamGrid = ParamGridBuilder().addGrid(dTree.maxDepth, [2,3,4,5,6]).build()\ncrossval = CrossValidator(estimator=dTree,\n                          estimatorParamMaps=paramGrid,\n                          evaluator=evaluator,\n                          numFolds=3)  # use 3+ folds in practice\n\ncrossval_rmse = CrossValidator(estimator=dTree,\n                          estimatorParamMaps=paramGrid,\n                          evaluator=evaluator1,\n                          numFolds=3)\n# Run cross-validation, and choose the best set of parameters.\ncvModel = crossval.fit(final_data)\ncvModel_rmse = crossval_rmse.fit(final_data)"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"code","source":["cvPredictions = cvModel.transform(final_data) \naccuracy = evaluator.evaluate(cvPredictions) \nprint(\"Model Accuracy with Cross Validation: \", accuracy*100)\ncvPredictions1 = cvModel_rmse.transform(final_data) \nrmse = evaluator1.evaluate(cvPredictions1) \nprint(\"RMSE: \", rmse)\n"],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"code","source":["from pyspark.ml.regression import (RandomForestRegressor)\nrfr = RandomForestRegressor(labelCol='target', featuresCol='features')"],"metadata":{},"outputs":[],"execution_count":64},{"cell_type":"code","source":["rfr_model = rfr.fit(train)\nrfr_pred = rfr_model.transform(test)\nevaluator = RegressionEvaluator(\n    labelCol=\"target\", predictionCol=\"prediction\", metricName=\"r2\")\naccuracy = evaluator.evaluate(rfr_pred)\nprint(\"Accuracy on test data = %g\" % (accuracy * 100))\n\nevaluator1 = RegressionEvaluator(\n    labelCol=\"target\", predictionCol=\"prediction\", metricName=\"rmse\")\nrmse = evaluator1.evaluate(rfr_pred)\nprint(\"RMSE = %g\" % (rmse))"],"metadata":{},"outputs":[],"execution_count":65},{"cell_type":"code","source":["paramGrid = ParamGridBuilder().addGrid(rfr.maxDepth, [2,3,4,5,6]).build()\ncrossval = CrossValidator(estimator=rfr,\n                          estimatorParamMaps=paramGrid,\n                          evaluator=evaluator,\n                          numFolds=3)  # use 3+ folds in practice\n\ncrossval_rmse = CrossValidator(estimator=rfr,\n                          estimatorParamMaps=paramGrid,\n                          evaluator=evaluator1,\n                          numFolds=3)\n\n# Run cross-validation, and choose the best set of parameters.\ncvModel = crossval.fit(final_data)\ncvModel_rmse = crossval_rmse.fit(final_data)\n\ncvPredictions = cvModel.transform(final_data) \naccuracy = evaluator.evaluate(cvPredictions) \nprint (\"Model Accuracy with Cross Validation: \", accuracy*100)\ncvPredictions1 = cvModel_rmse.transform(final_data) \nrmse = evaluator1.evaluate(cvPredictions1) \nprint(\"RMSE: \", rmse)"],"metadata":{},"outputs":[],"execution_count":66},{"cell_type":"code","source":["from pyspark.ml.regression import (GBTRegressor)\ngbr = GBTRegressor(labelCol='target', featuresCol='features')"],"metadata":{},"outputs":[],"execution_count":67},{"cell_type":"code","source":["GBR_model = gbr.fit(train)\nGBR_pred = GBR_model.transform(test)\nevaluator = RegressionEvaluator(\n    labelCol=\"target\", predictionCol=\"prediction\", metricName=\"r2\")\naccuracy = evaluator.evaluate(GBR_pred)\n\nprint(\"Accuracy on test data = %g\" % (accuracy * 100))\n\nevaluator1 = RegressionEvaluator(\n    labelCol=\"target\", predictionCol=\"prediction\", metricName=\"rmse\")\nrmse = evaluator1.evaluate(GBR_pred)\nprint(\"RMSE = %g\" % (rmse))"],"metadata":{},"outputs":[],"execution_count":68},{"cell_type":"code","source":["paramGrid = ParamGridBuilder().addGrid(gbr.maxDepth, [2,3,4,5,6]).build()\ncrossval = CrossValidator(estimator=gbr,\n                          estimatorParamMaps=paramGrid,\n                          evaluator=evaluator,\n                          numFolds=3)  # use 3+ folds in practice\n\ncrossval_rmse = CrossValidator(estimator=gbr,\n                          estimatorParamMaps=paramGrid,\n                          evaluator=evaluator1,\n                          numFolds=3)\n# Run cross-validation, and choose the best set of parameters.\ncvModel = crossval.fit(final_data)\ncvModel_rmse = crossval_rmse.fit(final_data)\n\ncvPredictions = cvModel.transform(final_data) \naccuracy = evaluator.evaluate(cvPredictions) \nprint (\"Model Accuracy with Cross Validation: \", accuracy*100)\ncvPredictions1 = cvModel_rmse.transform(final_data) \nrmse = evaluator1.evaluate(cvPredictions1) \nprint(\"RMSE: \", rmse)"],"metadata":{},"outputs":[],"execution_count":69},{"cell_type":"code","source":["randomforest = rfr_pred.toPandas()\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfig, ax = plt.subplots()\nax.plot(randomforest.prediction, label='Predicted')\nax.plot(randomforest.target, label='Target')\nax.set_xlabel('predicted date')\nax.set_ylabel('Number of trips')\nax.legend()\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":70},{"cell_type":"markdown","source":["References:\n1. https://forums.databricks.com/\n2. https://stackoverflow.com/questions/tagged/databricks"],"metadata":{}}],"metadata":{"name":"CIS PROJECT 5560 (FINAL VERSION)","notebookId":995953237688618},"nbformat":4,"nbformat_minor":0}
